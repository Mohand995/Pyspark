{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f97978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "٢٢/٠٧/١٣ ٢٣:١٢:٤٨ WARN Utils: Your hostname, mohand-Lenovo-ideapad-310-15IKB resolves to a loopback address: 127.0.1.1; using 192.168.1.17 instead (on interface wlp2s0)\n",
      "٢٢/٠٧/١٣ ٢٣:١٢:٤٨ WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "٢٢/٠٧/١٣ ٢٣:١٢:٤٨ WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201cee8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac8040",
   "metadata": {},
   "source": [
    "### Create the schema of the streamed files (check the column names and types from the CSV files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b794c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               StringType, IntegerType,DoubleType)\n",
    "\n",
    "recordSchema = StructType([StructField('_c0', IntegerType(), True),\n",
    "                           StructField('Date', StringType(), True),\n",
    "                           StructField('Open', DoubleType(), True),\n",
    "                           StructField('High', DoubleType(), True),\n",
    "                           StructField('Low', DoubleType(), True),\n",
    "                           StructField('Close',DoubleType(),True),\n",
    "                           StructField('Adj Close',DoubleType(),True),\n",
    "                          StructField('Volume',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50fbeb",
   "metadata": {},
   "source": [
    "### Create the dataframe by reading the stream using format \"csv\" and the schema you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77671b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"csv\") \\\n",
    "    .schema(recordSchema) \\\n",
    "    .load(\"kospi/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e0e44",
   "metadata": {},
   "source": [
    "### Make sure the sataframe is streaming the files from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d5a3a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d68de",
   "metadata": {},
   "source": [
    "### Create a stream writer into memory and specify the query name \"stock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f40bf944",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"memory\")  \\\n",
    "    .queryName('stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5f9a4",
   "metadata": {},
   "source": [
    "### Start the write stream and make sure it works (read all columns from the table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d98c6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٤ WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9df92afc-a15f-414e-bbcb-546eee8d22d9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٤ WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bc60ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from stock').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16908fa6",
   "metadata": {},
   "source": [
    "### Remove the first row from the data (hint: drop the rows where ALL values are null), then add a new column \"diff\", which is the difference between high and low columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2eaa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "+---+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from stock where Open is not null\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70dc2dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "692fc73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('diff',col('High')-col('Low'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0fb88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٧ ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7ee01d71 is aborting.\n",
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٧ ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@7ee01d71 aborted.\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14581b",
   "metadata": {},
   "source": [
    "### Create a new write stream using the new generated dataframe and call the generate table \"modified_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb1be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer2 = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"memory\")  \\\n",
    "    .queryName('modified_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969acbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٧ WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cb2afbb0-ff4b-4882-8963-9c1ed9b768bc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٧ WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query=writer2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3bcedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "|_c0|Date|Open|High|Low|Close|Adj Close|Volume|diff|\n",
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "+---+----+----+----+---+-----+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from modified_data').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c3036",
   "metadata": {},
   "source": [
    "### Write the generated data into files instead of the memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6441b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode(\"append\")\\\n",
    ".format(\"parquet\")\\\n",
    ".option(\"path\", \"Generated/\")\\\n",
    ".option(\"checkpointLocation\", \"chkpnt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ebe5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "٢٢/٠٧/١٣ ٢٣:١٢:٥٨ WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query=writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e02244ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f07e3f",
   "metadata": {},
   "source": [
    "### Stop the query. Now, try reading the generated parquet files into a normal dataframe\n",
    "- Create a schema and use it to read the data.\n",
    "- Show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec1321fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "finalDF= spark.read.format(\"csv\") \\\n",
    "    .schema(recordSchema) \\\n",
    "    .load(\"Generated/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d948b9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|_c0|      Date|        Open|        High|         Low|       Close|   Adj Close|Volume|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|240|2000-12-05|26585.300781|27367.300781|26372.099609|27011.800781|25526.091797| 91019|\n",
      "|241|2000-12-06|27011.800781|27509.400391|26798.599609|26869.699219|25391.804688|105791|\n",
      "|242|2000-12-07|27011.800781|27011.800781|26478.699219|26656.400391|25190.236328| 40656|\n",
      "|243|2000-12-08|26656.400391|27722.699219|26656.400391|27651.599609|26130.699219|149964|\n",
      "|244|2000-12-11|27687.099609|     28860.0|27651.599609|28078.099609|26533.740234|159671|\n",
      "|245|2000-12-12|28042.599609|28078.099609|27438.300781|27935.900391|26399.361328| 74560|\n",
      "|246|2000-12-13|27651.599609|     29286.5|27651.599609|28469.099609|26903.234375|270385|\n",
      "|247|2000-12-14|28469.099609|29784.099609|28291.300781|28362.400391| 26802.40625|256317|\n",
      "|248|2000-12-15|28362.400391|28895.599609|27793.800781|27935.900391|26399.361328|108886|\n",
      "|249|2000-12-18|     27580.5|     28433.5|27367.300781|28291.300781|26735.216797| 92848|\n",
      "|250|2000-12-19|27722.699219|28788.900391|27651.599609|27651.599609|26130.699219|115779|\n",
      "|251|2000-12-20|27402.800781|27793.800781|     27154.0|27722.699219|26197.890625| 99601|\n",
      "|252|2000-12-21|27367.300781|27793.800781|27082.900391|27722.699219|26197.890625|129706|\n",
      "|253|2000-12-22|27687.099609|     28007.0|27509.400391|     28007.0|26466.550781| 77233|\n",
      "|254|2000-12-26|     27971.5|31987.699219|27473.900391|     28433.5|26869.591797|131535|\n",
      "|255|2001-01-02|27367.300781|27367.300781|25945.599609|26656.400391|25190.236328| 64150|\n",
      "|256|2001-01-03|26585.300781|26656.400391|26016.699219|26656.400391|25190.236328| 37421|\n",
      "|257|2001-01-04|27367.300781|28078.099609|26905.199219|27082.900391|25593.277344|131254|\n",
      "|258|2001-01-05|26940.800781|26940.800781|26194.400391|26585.300781|25123.048828| 90316|\n",
      "|259|2001-01-08|26514.199219|27011.800781|26052.199219|26052.199219|24619.269531|102414|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814d697",
   "metadata": {},
   "source": [
    "### Sort the dataframe based on the ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c51d91a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|_c0|      Date|        Open|        High|         Low|       Close|   Adj Close|Volume|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "|  0|2000-01-04|22817.900391|25696.800781|22817.900391|24879.300781|23510.880859|108745|\n",
      "|  1|2000-01-05|24523.900391|26229.900391|23670.900391|24417.300781|23074.294922|175990|\n",
      "|  2|2000-01-06|24381.699219|24666.099609|22746.800781|22817.900391|21562.865234| 71746|\n",
      "|  3|2000-01-07|     22036.0|24879.300781|     22036.0|23884.199219|22570.513672|120984|\n",
      "|  4|2000-01-10|24879.300781|25519.099609|23813.099609|24061.900391|22738.439453|151371|\n",
      "|  5|2000-01-11|     24168.5|     25021.5|23955.199219|24239.599609|22906.365234| 95943|\n",
      "|  6|2000-01-12|     24168.5|24452.800781|23457.599609|23670.900391|22368.947266| 61899|\n",
      "|  7|2000-01-13|23670.900391|24132.900391|23102.199219|23244.400391| 21965.90625| 57538|\n",
      "|  8|2000-01-14|23457.599609|     24168.5|22746.800781|23244.400391| 21965.90625| 84267|\n",
      "|  9|2000-01-17|22533.599609|23457.599609|22533.599609|23457.599609|22167.376953| 67807|\n",
      "| 10|2000-01-18|23457.599609|     23742.0|22746.800781|23422.099609|22133.832031| 27995|\n",
      "| 11|2000-01-19|22817.900391|23173.300781|     22036.0|     22036.0|20823.970703| 44173|\n",
      "| 12|2000-01-20|21325.099609|22000.400391|     20756.5|21680.599609|20488.117188| 47550|\n",
      "| 13|2000-01-21|21680.599609|22391.400391|20863.099609|21680.599609|20488.117188| 80750|\n",
      "| 14|2000-01-24|20969.699219|21822.699219|20969.699219|20969.699219|19816.320313| 79906|\n",
      "| 15|2000-01-25|20258.900391|20934.199219|     19548.0|20116.699219|19010.236328|170925|\n",
      "| 16|2000-01-26|20223.300781|20543.199219|19761.300781|     20330.0|19211.804688| 59929|\n",
      "| 17|2000-01-27|     20401.0|22746.800781|     20330.0|21040.800781|19883.511719|139132|\n",
      "| 18|2000-01-28|21431.800781|22107.099609|21040.800781|21964.900391|20756.783203| 78640|\n",
      "| 19|2000-01-31|21325.099609|21893.800781|     21183.0|21467.300781|20286.552734| 45861|\n",
      "+---+----------+------------+------------+------------+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalDFSorted = finalDF.sort('_c0')\n",
    "finalDFSorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218d5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
